% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/blurr_hugging_face.R
\name{HF_QABatchTransform}
\alias{HF_QABatchTransform}
\title{HF_QABatchTransform}
\usage{
HF_QABatchTransform(
  hf_arch,
  hf_tokenizer,
  max_length = NULL,
  padding = TRUE,
  truncation = TRUE,
  is_split_into_words = FALSE,
  n_tok_inps = 1,
  hf_input_return_type = HF_QuestionAnswerInput(),
  ...
)
}
\arguments{
\item{hf_arch}{architecture}

\item{hf_tokenizer}{tokenizer}

\item{max_length}{maximum length}

\item{padding}{padding}

\item{truncation}{truncation}

\item{is_split_into_words}{to split into words or not}

\item{n_tok_inps}{number of tok inputs}

\item{hf_input_return_type}{input return type}

\item{...}{additional arguments}
}
\value{
None
}
\description{
Handles everything you need to assemble a mini-batch of inputs and targets,
as well as decode the dictionary produced
}
\details{
as a byproduct of the tokenization process in the `encodes` method.
}
